<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Ansible Collaborative (Posts by Leonardo Araujo)</title><link>https://ansible.com/</link><description></description><atom:link href="https://ansible.com/authors/leonardo-araujo.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2024 &lt;a href="mailto:website@ansible.community"&gt;Ansible Collaborative, et al&lt;/a&gt; </copyright><lastBuildDate>Thu, 21 Mar 2024 14:38:24 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Centralize your Automation Logs with Ansible Tower and Splunk Enterprise</title><link>https://ansible.com/blog/centralize-your-automation-logs-with-ansible-tower-and-splunk-enterprise/</link><dc:creator>Leonardo Araujo</dc:creator><description>&lt;h2&gt;Centralize your Automation Logs with Ansible Tower and Splunk Enterprise&lt;/h2&gt;
&lt;p&gt;For many IT teams, automation is a core component these days. But
automation is not something on it's own - it is a part of a puzzle and
needs to interact with the surrounding IT. So one way to grade
automation is how well it integrates with other tooling of the IT
ecosystem - like the central logging infrastructure. After all, through
the central logging the IT team can quickly survey what is happening,
where, and what the state of it is.&lt;/p&gt;
&lt;p&gt;The Red Hat Ansible Automation Platform is a solution to build and
operate automation at scale. As part of the platform, Ansible Tower
integrates well with external logging solutions, such as Splunk, and it
is easy to set that up. In this blog post we will demonstrate how to
perform the necessary configurations in both Splunk and Ansible Tower to
let them work well together.&lt;/p&gt;
&lt;h3&gt;Setup of Splunk&lt;/h3&gt;
&lt;p&gt;The first step is to get Splunk up and running. You can download a
Splunk RPM after you register yourself at the &lt;a href="https://www.splunk.com/en_us"&gt;Splunk home
page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;After the registration, download the rpm and perform the installation:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;$&lt;span class="w"&gt; &lt;/span&gt;rpm&lt;span class="w"&gt; &lt;/span&gt;-ivh&lt;span class="w"&gt; &lt;/span&gt;splunk-8.0.3-a6754d8441bf-linux-2.6-x86_64.rpm
warning:&lt;span class="w"&gt; &lt;/span&gt;splunk-8.0.3-a6754d8441bf-linux-2.6-x86_64.rpm:&lt;span class="w"&gt; &lt;/span&gt;Header&lt;span class="w"&gt; &lt;/span&gt;V4&lt;span class="w"&gt; &lt;/span&gt;RSA/SHA256&lt;span class="w"&gt; &lt;/span&gt;Signature,&lt;span class="w"&gt; &lt;/span&gt;key&lt;span class="w"&gt; &lt;/span&gt;ID&lt;span class="w"&gt; &lt;/span&gt;b3cd4420:&lt;span class="w"&gt; &lt;/span&gt;NOKEY
Verifying...&lt;span class="w"&gt;                    &lt;/span&gt;&lt;span class="c1"&gt;################################# [100%]&lt;/span&gt;
Preparing...&lt;span class="w"&gt;                    &lt;/span&gt;&lt;span class="c1"&gt;################################# [100%]&lt;/span&gt;
Updating&lt;span class="w"&gt; &lt;/span&gt;/&lt;span class="w"&gt; &lt;/span&gt;installing...
&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;:splunk-8.0.3-a6754d8441bf&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="c1"&gt;################################# [100%]&lt;/span&gt;
&lt;span class="nb"&gt;complete&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;After the installation is complete, execute the command below to start
the service and make the necessary settings.&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;$&lt;span class="w"&gt;  &lt;/span&gt;/opt/splunk/bin/splunk&lt;span class="w"&gt; &lt;/span&gt;start&lt;span class="w"&gt; &lt;/span&gt;-accept-license
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Accept the terms, set the username and password, and wait for the
service to start.&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;All&lt;span class="w"&gt; &lt;/span&gt;preliminary&lt;span class="w"&gt; &lt;/span&gt;checks&lt;span class="w"&gt; &lt;/span&gt;passed.

Starting&lt;span class="w"&gt; &lt;/span&gt;splunk&lt;span class="w"&gt; &lt;/span&gt;server&lt;span class="w"&gt; &lt;/span&gt;daemon&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;splunkd&lt;span class="o"&gt;)&lt;/span&gt;...
Done
&lt;span class="w"&gt;                                                        &lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;OK&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;

Waiting&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;web&lt;span class="w"&gt; &lt;/span&gt;server&lt;span class="w"&gt; &lt;/span&gt;at&lt;span class="w"&gt; &lt;/span&gt;http://127.0.0.1:8000&lt;span class="w"&gt; &lt;/span&gt;to&lt;span class="w"&gt; &lt;/span&gt;be&lt;span class="w"&gt; &lt;/span&gt;available...&lt;span class="w"&gt; &lt;/span&gt;Done


If&lt;span class="w"&gt; &lt;/span&gt;you&lt;span class="w"&gt; &lt;/span&gt;get&lt;span class="w"&gt; &lt;/span&gt;stuck,&lt;span class="w"&gt; &lt;/span&gt;we&lt;span class="err"&gt;'&lt;/span&gt;re&lt;span class="w"&gt; &lt;/span&gt;here&lt;span class="w"&gt; &lt;/span&gt;to&lt;span class="w"&gt; &lt;/span&gt;help.
Look&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;answers&lt;span class="w"&gt; &lt;/span&gt;here:&lt;span class="w"&gt; &lt;/span&gt;http://docs.splunk.com

The&lt;span class="w"&gt; &lt;/span&gt;Splunk&lt;span class="w"&gt; &lt;/span&gt;web&lt;span class="w"&gt; &lt;/span&gt;interface&lt;span class="w"&gt; &lt;/span&gt;is&lt;span class="w"&gt; &lt;/span&gt;at&lt;span class="w"&gt; &lt;/span&gt;http://splunk-server:8000
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Access the web interface and enter the username and password. &lt;/p&gt;
&lt;h3&gt;Configuring Data Input with Red Hat Ansible Content Collections&lt;/h3&gt;
&lt;p&gt;To receive the Ansible Tower logs in Splunk, we need to create a Data
Input TCP. To do that we will use the &lt;a href="https://cloud.redhat.com/ansible/automation-hub/splunk/es"&gt;Splunk Enterprise Security
Content
Collection&lt;/a&gt;
available on Automation Hub as part of the Red Hat-Maintained Content
Collections release.&lt;/p&gt;
&lt;p&gt;This Collection has been created to support Splunk Enterprise Security,
a security product delivered as an add-on application for Splunk
Enterprise and extends that to deliver Security Information and Event
Management (SIEM) functionalities. Splunk Enterprise Security leverages
many capabilities of the underlying platform hence, despite having been
developed for security automation use cases, most of the modules in this
Collection can be used to support Day 0 and  Day 1 IT Operations use
cases as well. If you want to read more about how Ansible Content
Collections developed as part of the Ansible security automation
initiative can help to overcome security operation challenges, check out
our blog post "Getting started with Ansible security automation: investigation enrichment"
from our Roland Wolters.&lt;/p&gt;
&lt;p&gt;The Splunk Enterprise Security Content Collection has the following
modules as of today:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;adaptive_response_notable_event&lt;/code&gt; - Manage Splunk Enterprise Security Notable Event Adaptive Responses&lt;/li&gt;
&lt;li&gt;&lt;code&gt;correlation_search&lt;/code&gt; - Manage Splunk Enterprise Security Correlation Searches&lt;/li&gt;
&lt;li&gt;&lt;code&gt;correlation_search_info&lt;/code&gt; - Manage Splunk Enterprise Security Correlation Searches&lt;/li&gt;
&lt;li&gt;&lt;code&gt;data_input_monitor&lt;/code&gt; - Manage Splunk Data Inputs of type Monitor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;data_input_network&lt;/code&gt; - Manage Splunk Data Inputs of type TCP or UDP&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you want to learn more about collections in general and how to get
started with them, check out our blog post "Hands on with Ansible
collections" from our Ajay Chenampara.&lt;/p&gt;
&lt;p&gt;Coming back to our use case, we will use the data_input_network module.
First let's install the Collection &lt;code&gt;splunk.es&lt;/code&gt;:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;$&lt;span class="w"&gt; &lt;/span&gt;ansible-galaxy&lt;span class="w"&gt; &lt;/span&gt;collection&lt;span class="w"&gt; &lt;/span&gt;install&lt;span class="w"&gt; &lt;/span&gt;splunk.es
Process&lt;span class="w"&gt; &lt;/span&gt;install&lt;span class="w"&gt; &lt;/span&gt;dependency&lt;span class="w"&gt; &lt;/span&gt;map
Starting&lt;span class="w"&gt; &lt;/span&gt;collection&lt;span class="w"&gt; &lt;/span&gt;install&lt;span class="w"&gt; &lt;/span&gt;process
Installing&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s1"&gt;'splunk.es:1.0.0'&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;to&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s1"&gt;'/root/.ansible/collections/ansible_collections/splunk/es'&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;After the installation of the Collection, the next step is to create our inventory:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="k"&gt;[splunk]&lt;/span&gt;
&lt;span class="na"&gt;splunk.customer.com&lt;/span&gt;

&lt;span class="k"&gt;[splunk:vars]&lt;/span&gt;
&lt;span class="na"&gt;ansible_network_os&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;splunk.es.splunk&lt;/span&gt;
&lt;span class="na"&gt;ansible_user&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;USER&lt;/span&gt;
&lt;span class="na"&gt;ansible_httpapi_pass&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;PASS&lt;/span&gt;
&lt;span class="na"&gt;ansible_httpapi_port&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;8089&lt;/span&gt;
&lt;span class="na"&gt;ansible_httpapi_use_ssl&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;yes&lt;/span&gt;
&lt;span class="na"&gt;ansible_httpapi_validate_certs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;True&lt;/span&gt;
&lt;span class="na"&gt;ansible_connection&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;httpapi&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Note that we set the connection type to httpapi: the communication with
Splunk Enterprise Security takes place via REST API. Also, remember to
adjust the authentication, port and certificate data according to your
environment.&lt;/p&gt;
&lt;p&gt;Next let's create the playbook which will set up the input network:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="nn"&gt;---&lt;/span&gt;
&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Splunk Data Input&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;hosts&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;splunk&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;gather_facts&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;False&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;collections&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;splunk.es&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;tasks&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;create splunk_data_input_network&lt;/span&gt;
&lt;span class="w"&gt;      &lt;/span&gt;&lt;span class="nt"&gt;splunk.es.data_input_network&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;"9199"&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nt"&gt;protocol&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;"tcp"&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nt"&gt;source&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;"http:tower_logging_collections"&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nt"&gt;sourcetype&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;"httpevent"&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nt"&gt;state&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;"present"&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Let's run the playbook to create the input network:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;$&lt;span class="w"&gt; &lt;/span&gt;ansible-playbook&lt;span class="w"&gt; &lt;/span&gt;-i&lt;span class="w"&gt; &lt;/span&gt;inventory.ini&lt;span class="w"&gt; &lt;/span&gt;splunk_with_collections.yml
&lt;/pre&gt;&lt;/div&gt;

&lt;h3&gt;Validating Data Input&lt;/h3&gt;
&lt;p&gt;To validate if our data input was created, in the Splunk web interface,
click on &lt;strong&gt;Settings -&amp;gt; Data inputs -&amp;gt; TCP&lt;/strong&gt;. Verify that the TCP port is
listed as a source type "httpevent" like in the screenshot below:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Splunk blog one" src="https://ansible.com/images/posts/archive/splunk-blog-one.png"&gt;&lt;/p&gt;
&lt;p&gt;We can also validate the data input by checking if the port 9199 is open
and does receive connections:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;$&lt;span class="w"&gt;  &lt;/span&gt;telnet&lt;span class="w"&gt; &lt;/span&gt;splunk.customer.com&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;9199&lt;/span&gt;
Trying&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;.2.3.4...
Connected&lt;span class="w"&gt; &lt;/span&gt;to&lt;span class="w"&gt; &lt;/span&gt;splunk.customer.com.
Escape&lt;span class="w"&gt; &lt;/span&gt;character&lt;span class="w"&gt; &lt;/span&gt;is&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s1"&gt;'^]'&lt;/span&gt;.
&lt;/pre&gt;&lt;/div&gt;

&lt;h3&gt;Configuring Ansible Tower&lt;/h3&gt;
&lt;p&gt;The activity stream logs in Ansible Tower provide information on
creating and deleting objects, such as logging activities within the
Ansible Tower, for more information and details, check out &lt;a href="https://docs.ansible.com/ansible-tower/latest/html/administration/logging.html"&gt;the
documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;After Splunk is all set up, let's dive into Ansible Tower, and connect
both tools with each other! First we are going to configure Ansible
Tower to send logs to Data Input  in Splunk. For this, we enter the
Ansible Tower Settings: there, pick "System" and click  "Logging". This
opens an overview of the logging configuration of Ansible Tower as shown
below. In there, we specify the URL for Splunk as well as the URL
context &lt;code&gt;/services/collector/event&lt;/code&gt;. Also, we have to provide the
port, here 9199, and select the right aggregator type, here Splunk. Now
select protocol TCP, and click first the "Save" button and then, to
verify our configuration, the "Test" button.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Splunk blog two" src="https://ansible.com/images/posts/archive/splunk-blog-two.png"&gt;&lt;/p&gt;
&lt;h3&gt;Viewing the logs in Splunk&lt;/h3&gt;
&lt;p&gt;Now that Ansible Tower is all set up, let's head back to Splunk and
check if the logs are making their way there. In Splunk home, click on
"Search &amp;amp; Reporting". In "What to Search" pick "Data Summary". A window
will open up, where you can click on the "Sources" column:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Splunk blog three" src="https://ansible.com/images/posts/archive/splunk-blog-three.png"&gt;&lt;/p&gt;
&lt;p&gt;Click on the source http:tower_logging_collection, this will take us to
the Search screen, where it is possible to view the records received
from Ansible Tower:&lt;/p&gt;
&lt;p&gt;&lt;img alt="splunk blog" src="https://ansible.com/images/posts/archive/splunk-blog-four.png"&gt;&lt;/p&gt;
&lt;p&gt;If all is working fine, you should see the last log events received from
Ansible Tower, showing that the two tools are now properly connected.
Congratulations!&lt;/p&gt;
&lt;p&gt;But we don't want to stop there: after all, logging is all about
analyzing the incoming information and making sense of it. So let's
create a filter: click on the field you'd like to filter, to be filtered
and then pick "Add to search".&lt;/p&gt;
&lt;p&gt;&lt;img alt="splunk blog five" src="https://ansible.com/images/posts/archive/splunk-blog-five.png"&gt;&lt;/p&gt;
&lt;p&gt;After that, the search field will be filled with our ﬁlter.&lt;/p&gt;
&lt;p&gt;&lt;img alt="splunk blog six" src="https://ansible.com/images/posts/archive/splunk-blog-six.png"&gt;&lt;/p&gt;
&lt;h3&gt;Creating a simple dashboard&lt;/h3&gt;
&lt;p&gt;In this example, we will create a simple graph of the events generated
by Ansible Tower.&lt;/p&gt;
&lt;p&gt;We will use the previous step on how to create a filter, but this time
we will filter the event field and in the search field we will leave it
this way:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="nv"&gt;source&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"http:tower_logging_collection"&lt;/span&gt;&lt;span class="p"&gt;|&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;spath&lt;span class="w"&gt; &lt;/span&gt;event&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;|&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;search&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;event&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;*
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With &lt;code&gt;event = *&lt;/code&gt; all events are filtered.  After that click on the
"All Fields" button on the left side menu, select the event field and
click on exit. That done, click on Visualization and then select the
Pivot option, in the window select "Selected Fields (1)" and click OK.&lt;/p&gt;
&lt;p&gt;&lt;img alt="splunk blog seven" src="https://ansible.com/images/posts/archive/splunk-blog-seven.png"&gt;&lt;/p&gt;
&lt;p&gt;In this window, we will keep the filters as "All time", in "Split
Columns" select event and then "Add To Table", after that we can
already have a view of the information separated in columns with the
name of the column being the event and their number of appearances in
the logs.&lt;/p&gt;
&lt;p&gt;&lt;img alt="splunk blog eight" src="https://ansible.com/images/posts/archive/splunk-blog-eight.png"&gt;&lt;/p&gt;
&lt;p&gt;After viewing the information in columns, click "Save As" and select
"Dashboard Panel".  In "Dashboard" select "New", in "Dashboard
Title" define the name you want for the Dashboard, this name will
generate the Dashboard ID, in Panel Title and Model Title, define the
name of this search, for example all_events and click Save and then View
Dashboard.&lt;/p&gt;
&lt;p&gt;&lt;img alt="splunk blog nine" src="https://ansible.com/images/posts/archive/splunk-blog-nine.png"&gt;&lt;/p&gt;
&lt;p&gt;In the following screen, click on Edit in the upper right menu then in
the all_events panel click on "Select Visualization", choose the
visualization you want, in this example we select "Bar Chart" and click
"Save".&lt;/p&gt;
&lt;p&gt;&lt;img alt="splunk blog ten" src="https://ansible.com/images/posts/archive/splunk-blog-ten.png"&gt;&lt;/p&gt;
&lt;p&gt;Now that we have our dashboard with a chart listing all events, repeat
the process of creating filters and in saving the search, select an
existing dashboard to add new panels to the dashboard we created.&lt;/p&gt;
&lt;p&gt;After creating some panels and adding them to the existing dashboard, we
will have a visualization like this:&lt;/p&gt;
&lt;p&gt;&lt;img alt="splunk blog eleven" src="https://ansible.com/images/posts/archive/splunk-blog-eleven.png"&gt;&lt;/p&gt;
&lt;p&gt;To use more advanced features of integrating Ansible Tower with Splunk,
see the Collection
&lt;a href="https://cloud.redhat.com/ansible/automation-hub/splunk/enterprise_security"&gt;Splunk_enterprise_security,&lt;/a&gt;
which will allow you to configure Data Inputs and search correlation
options, among other features.&lt;/p&gt;
&lt;h3&gt;Takeaways and where to go next&lt;/h3&gt;
&lt;p&gt;In this post, we demonstrate how to send the Ansible Tower usage logs to
Splunk to enable a centralized view of all events generated by Ansible
Tower. That way we can create graphs from various information, such as
the number of playbooks that failed or succeeded, modules most used in
the executed playbooks and so on.&lt;/p&gt;</description><guid>https://ansible.com/blog/centralize-your-automation-logs-with-ansible-tower-and-splunk-enterprise/</guid><pubDate>Thu, 09 Jul 2020 00:00:00 GMT</pubDate></item><item><title>Red Hat Ansible Tower Monitoring Using Prometheus, Node Exporter, and Grafana</title><link>https://ansible.com/blog/red-hat-ansible-tower-monitoring-using-prometheus-node-exporter-grafana/</link><dc:creator>Leonardo Araujo</dc:creator><description>&lt;h2&gt;Red Hat Ansible Tower Monitoring Using Prometheus, Node Exporter, and Grafana&lt;/h2&gt;
&lt;p&gt;A crucial piece of automation is ensuring that it runs flawlessly.
Automation Analytics can help by providing insight into health state and organizational
statistics. However, there is often the need to monitor the current
state of  Ansible Tower. Luckily, Ansible Tower does provide metrics via
the API, and they can easily be fed into Grafana.&lt;/p&gt;
&lt;p&gt;This blog post will outline how to monitor Ansible Tower environments by
feeding Ansible Tower and operating system metrics into Grafana by using
node_exporter &amp;amp; Prometheus.&lt;/p&gt;
&lt;p&gt;To reach that goal we configure Ansible Tower metrics for Prometheus to
be viewed via Grafana and we will use node_exporter to export the
operating system metrics to an operating system (OS)  dashboard in
Grafana. Note that we use Red Hat Enterprise Linux 8 as the OS running
Ansible Tower here. The data flow is outlined below:&lt;/p&gt;
&lt;p&gt;&lt;img alt="analytics data flow diagram" src="https://ansible.com/images/posts/archive/analytics-data-flow-diagram.png"&gt;&lt;/p&gt;
&lt;p&gt;As you see, Grafana looks for data in Prometheus. Prometheus itself
collects the data in its database by importing them from node_exporters
and from the Ansible Tower APIs.&lt;/p&gt;
&lt;p&gt;In this blog post we assume a cluster of three Ansible Tower instances
and an external database. Also please note that this blog post assumes
an already installed instance of Prometheus and Grafana.&lt;/p&gt;
&lt;h2&gt;Setup  of node_exporter&lt;/h2&gt;
&lt;p&gt;As a first step we will set up node_exporter on the Ansible Tower
servers and the external database. Since node_exporter is not available
in Red Hat Enterprise Linux 8 by default we first have to install it. To
do that we login to our Ansible Tower server, clone the corresponding
git repository and change into the repository directory. See the listing
shown below for reference:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;$&lt;span class="w"&gt; &lt;/span&gt;git&lt;span class="w"&gt; &lt;/span&gt;clone&lt;span class="w"&gt; &lt;/span&gt;https://github.com/redhat-cop/tower_grafana_dashboards&lt;span class="w"&gt; &lt;/span&gt;

$&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;cd&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;tower_grafana_dashboards/

$&lt;span class="w"&gt; &lt;/span&gt;tree
.
├──&lt;span class="w"&gt; &lt;/span&gt;install_node_exporter.yaml
├──&lt;span class="w"&gt; &lt;/span&gt;metric_servers.json
└──&lt;span class="w"&gt; &lt;/span&gt;metric_tower.json

&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;directories,&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;files
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Next, we have to perform the actual installation of node_exporter.
Luckily, a playbook to install it is included. Run the
install_node_exporter.yaml playbook to perform the installation of
node_exporter. &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;$&lt;span class="w"&gt; &lt;/span&gt;ansible-playbook&lt;span class="w"&gt; &lt;/span&gt;install_node_exporter.yaml
...
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The output of the playbook is shown below:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Analytics blog 2" src="https://ansible.com/images/posts/archive/analytics-blog-two.png"&gt;&lt;/p&gt;
&lt;p&gt;After the installation, verify if node_exporter is indeed running and
listens on port 9100. This can easily done with netstat:&lt;/p&gt;
&lt;p&gt;&lt;img alt="analytics blog 3" src="https://ansible.com/images/posts/archive/analytics-blog-three.png"&gt;&lt;/p&gt;
&lt;p&gt;Repeat these steps on the other Ansible Tower servers as well as on the
external database.&lt;/p&gt;
&lt;h2&gt;Validating Ansible Tower metrics&lt;/h2&gt;
&lt;p&gt;Next let's shift our focus towards Ansible Tower. Validate that the
Ansible Tower metrics are being displayed correctly by accessing the url
below:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;https://tower.customer.com/api/v2/metrics
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Accessing the url we should see a listing of all available Ansible Tower
metrics, as shown below:&lt;/p&gt;
&lt;p&gt;&lt;img alt="analytics blog 4" src="https://ansible.com/images/posts/archive/analytics-blog-four.png"&gt;&lt;/p&gt;
&lt;p&gt;Let's  set up Prometheus to gather these data. First we need to generate
an &lt;a href="https://docs.ansible.com/ansible-tower/latest/html/administration/oauth2_token_auth.html"&gt;authentication token on Ansible
Tower&lt;/a&gt;:
the token will grant access to Ansible Tower without the need to enter
username and passwords each time it is accessed.&lt;/p&gt;
&lt;p&gt;To generate the token, access the Ansible Tower console and click on
your username that appears at the top of the page. From there, click on
"&lt;strong&gt;Tokens"&lt;/strong&gt; and then on the + sign. A new window pops up where you can
define the specifics of the token and finally create it, see the image
below. Choose the scope "read" and click the green "SAVE" button.&lt;/p&gt;
&lt;p&gt;&lt;img alt="analytics blog 5" src="https://ansible.com/images/posts/archive/analytics-blog-five.png"&gt;&lt;/p&gt;
&lt;h2&gt;Setting up Prometheus to receive metrics&lt;/h2&gt;
&lt;p&gt;With the token in our hands, we can now configure Prometheus, adding the
node_exporters scrape config and the scrape for Ansible Tower's
metrics. Open the configuration of your Prometheus installation with an
editor of your choice: &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;$&lt;span class="w"&gt; &lt;/span&gt;vim&lt;span class="w"&gt; &lt;/span&gt;/etc/prometheus/prometheus.yml
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Next, add the configuration for Ansible Tower and the operating system.
Below is an example:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="c1"&gt;## Scrape Config - Tower&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;job_name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;'tower'&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;metrics_path&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;/api/v2/metrics&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;scrape_interval&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;5s&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;scheme&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;https&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;bearer_token&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;xxxxxxxxxxxxxxxx (your bearer token)&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;static_configs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;targets&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;      &lt;/span&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;tower.customer.com&lt;/span&gt;

&lt;span class="c1"&gt;## Add Node Exporter&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;job_name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;'tower-01'&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;scrape_interval&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;5s&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;static_configs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;targets&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'172.31.66.203:9100'&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;job_name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;'tower-02'&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;scrape_interval&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;5s&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;static_configs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;targets&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'172.31.65.135:9100'&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;job_name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;'tower-db-01'&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;scrape_interval&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;5s&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;static_configs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;targets&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'172.31.64.218:9100'&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Note that the metrics for Ansible Tower are only collected once, while
the operating system metrics are collected for each server: Ansible
Tower helps ensure that all internal metrics are already collected and
shared among all installed servers of the cluster. But each operating
system on each server is independent and thus has independent OS
metrics.&lt;/p&gt;
&lt;p&gt;Restart Prometheus to apply the changes:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;$&lt;span class="w"&gt; &lt;/span&gt;systemctl&lt;span class="w"&gt; &lt;/span&gt;restart&lt;span class="w"&gt; &lt;/span&gt;prometheus
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now, access the url http://prometheus.customer.com/targets to validate
that the data are scraped properly. Ensure that , all endpoints are in
UP status as shown below:&lt;/p&gt;
&lt;p&gt;&lt;img alt="analytics blog 6" src="https://ansible.com/images/posts/archive/analytics-blog-six.png"&gt;&lt;/p&gt;
&lt;h2&gt;Grafana configuration to import the dashboards&lt;/h2&gt;
&lt;p&gt;Now let's import the dashboards into Grafana. Grafana can be configured
through json files. In the repo mentioned above we provide two json
files to configure two dashboards: metric_servers.json for the OS
metrics, and metric_tower.json for the Ansible Tower metrics. Let's
import them into Grafana to enable the dashboards.&lt;/p&gt;
&lt;p&gt;To do that, access your Grafana installation and click on the + sign in
the navigation menu on the left side. Pick &lt;strong&gt;"Folder"&lt;/strong&gt;,  enter a
desired name and create it.&lt;/p&gt;
&lt;p&gt;Afterwards we have the option to &lt;strong&gt;"Manage Dashboards"&lt;/strong&gt;, from where we
can import the prepared json file via upload&lt;strong&gt;.&lt;/strong&gt; Select the json file
metric_tower.json, choose the just created folder, change the uid and
choose the datasource as Prometheus as shown below:&lt;/p&gt;
&lt;p&gt;&lt;img alt="analytics blog 7" src="https://ansible.com/images/posts/archive/analytics-blog-seven.png"&gt;&lt;/p&gt;
&lt;p&gt;Initiate the import by pressing the corresponding button. After the
import of metric_tower.json is finished, we repeat the same process for
the metric_servers.json file.&lt;/p&gt;
&lt;h2&gt;The new Grafana dashboards&lt;/h2&gt;
&lt;p&gt;Once both uploads are finished, we can view the imported dashboards:&lt;/p&gt;
&lt;p&gt;&lt;img alt="analytics blog 8" src="https://ansible.com/images/posts/archive/analytics-blog-eight.png"&gt;&lt;/p&gt;
&lt;p&gt;In this Ansible Tower metrics dashboard, you can now see the following
information:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ansible Tower Version &lt;/li&gt;
&lt;li&gt;Ansible Automation Platform Version&lt;/li&gt;
&lt;li&gt;number of tower nodes&lt;/li&gt;
&lt;li&gt;number of hosts available in the license&lt;/li&gt;
&lt;li&gt;number of hosts used&lt;/li&gt;
&lt;li&gt;total users&lt;/li&gt;
&lt;li&gt;jobs successful&lt;/li&gt;
&lt;li&gt;jobs failed&lt;/li&gt;
&lt;li&gt;quantity by type of job execution&lt;/li&gt;
&lt;li&gt;graphics showing the number of jobs running and pending jobs&lt;/li&gt;
&lt;li&gt;graph showing the growth of the tool showing the amount of workflow,
    hosts, inventories, jobs, projects, organizations, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the Operating System metrics dashboard, we have the following
information:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Uptime&lt;/li&gt;
&lt;li&gt;total vcpus&lt;/li&gt;
&lt;li&gt;total memory&lt;/li&gt;
&lt;li&gt;cpu iowait&lt;/li&gt;
&lt;li&gt;memory consumption&lt;/li&gt;
&lt;li&gt;cpu busy&lt;/li&gt;
&lt;li&gt;Swap&lt;/li&gt;
&lt;li&gt;filesystem consumption&lt;/li&gt;
&lt;li&gt;disk iops&lt;/li&gt;
&lt;li&gt;system load&lt;/li&gt;
&lt;li&gt;used space graph&lt;/li&gt;
&lt;li&gt;graphics with disk writing and reading, network traffic and network
    sockets.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="analytics blog 9" src="https://ansible.com/images/posts/archive/analytics-blog-nine.png"&gt;&lt;/p&gt;
&lt;h2&gt;Takeaways and where to go next&lt;/h2&gt;
&lt;p&gt;In this post, we demonstrate how to create a monitoring of your Ansible
Tower environment using node_exporter to export metrics from the OS and
Prometheus collecting the metrics of the Ansible Tower api, we include
the OS consumption dashboards and Ansible Tower metrics, so that you
have a view more managerial of your environment, such as capacity,
licensing and jobs in execution, using graphics and counters, you can
identify problems and take actions quickly.&lt;/p&gt;
&lt;p&gt;If you're interested in detailed views across your entire automation
environment, you can also try Automation Analytics on
&lt;a href="https://cloud.redhat.com/"&gt;cloud.redhat.com&lt;/a&gt;.&lt;/p&gt;</description><guid>https://ansible.com/blog/red-hat-ansible-tower-monitoring-using-prometheus-node-exporter-grafana/</guid><pubDate>Wed, 06 May 2020 00:00:00 GMT</pubDate></item></channel></rss>